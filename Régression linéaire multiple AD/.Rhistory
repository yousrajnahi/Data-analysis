#Chargement du jeu des données
data <- read.csv("projectdata.csv")
data$Index <-NULL
#Visualisation du jeu des données
View(data)
#============================== modèle de régression linéaire multiple ============================================
#Calcule du modèle de régression linéaire multiple
model = lm(B~., data = data)
#Coefficients du modèle
coef(model) 
#Sommaire du modèle
summary(model) 
#Les variables qui ont une valeur de Pr(>|t|) >= 5% sont des variables non significatives
#Valeur de R^2 
summary(model)$r.squared
#Valeur de Raju^2
summary(model)$adj.r.squared
#============================================== Procédure step =======================================================
#Amélioration du modèle initiale par la procédure step
modelstep <- step(model)
#sommaire du modèle
summary(modelstep) 
################ Tests de validation 
# Test d'homoscédasticité
plot(predict(modelstep), resid(modelstep))
abline(h=0)
#Test de Normalité: shapiro
shapiro.test(resid(modelstep))
##Test de Normalité: ks
ks.test(resid(modelstep),pnorm) 
#====================== Application de la méthode pas à pas de sélection des variables explicatives ===========================
#Etape 0
# On commence par intégrer la variable la plus significative (Fisher le plus grand)
nva=ncol(data)-1
Fish = rep(0,nva)
p=ncol(data)-1
cible_col=16
precision_del=0.10 #10%
precision_aj=0.10 #10%
for (i in  1:p ) {
mod1<-lm(data[,cible_col]~data[,i])
Fish[i]=var(predict(mod1))*(nrow(data)-1)/(deviance(mod1)/df.residual(mod1))
}
df2=nrow(data)-2
V=match(max(Fish),Fish):match(max(Fish),Fish)
print('Ajout :')
print('Fisher :')
print(Fish) # Toutes les valeurs de Fisher
print('Max Fisher :')
print( max(Fish) ) # La valeur la plus grande de Fisher
print('p value :')
print((  1-pf(max(Fish),1,df2) ))
print('pos :') 
print( match(max(Fish),Fish) )# Position de la variable la plus significative
t=0
for(i in V){
if(t==0){
dev=cbind(data[,i])
t=1
}
else{
dev=cbind(dev,data[,i])
}
}
mod<-lm(data[,16]~dev)
print("R squared aj")
print(summary(mod)$adj.r.squared) #Valeur de Raju^2 
lastaj_pvalue=0
i=0
break_=0
#A chaque étape : la variable entrante est celle qui présente le plus grand Ficher avec p-value < 10% et la variable sortante est celle qui présente le plus petit Fisher avec p-value > 10%
while(1){
#Ajout
Fish = rep(0,nva)
t=0
for(i in V){
if(t==0){
dev=cbind(data[,i])
t=1
}
else{
dev=cbind(dev,data[,i])
}
}
SCR1<-deviance(lm(data[,cible_col]~dev))
for (i in 1:p) {
mod<-lm(data[,cible_col]~cbind(dev,data[,i]))
SCR2=deviance(mod)
Fish[i]=(SCR1-SCR2)/(SCR2/(nrow(data)-length(V)-2))
}
df2=nrow(data)-length(V)-2
#Ajout
if((1-pf(max(Fish),1,df2))<precision_aj){
print('Ajout :')
print('Fisher :')
print(Fish) # Toutes les valeurs de Fisher
print('Max Fisher :')
print( max(Fish) )# La valeur la plus grande de Fisher
print('p value :')
print((  1-pf(max(Fish),1,df2) )) #p-value de la variable entrante
print('pos :')
print( match(max(Fish),Fish) )# Position de la variable entrante
V[length(V)+1]=match(max(Fish),Fish)
print(V)
break_=0
}
else{
print('Ajout :')
print('Nothing') #Pas de valeur entrante
print('Fisher :')
print(Fish) # Toutes les valeurs de Fisher
print('Max Fisher :')
print( max(Fish) ) # La valeur la plus grande de Fisher
print('p value :')
print((  1-pf(max(Fish),1,df2) )) #p-value de la dernière variable entrante
print('pos :')
print( match(max(Fish),Fish) ) # Position de la dernière variable entrante
lastaj_pvalue=1-pf(max(Fish),1,df2)
break_=break_+1
}
#Retrait
Fish = rep(0,length(V))
t=0
for(i in V){
if(t==0){
dev=data[,i]
t=1
}
else{
dev=cbind(dev,data[,i])
}
}
SCR3=deviance(lm(data[,cible_col]~dev))
i=1
for(j in 1:length(V)){
m=0
for(z in 1:length(V)){
if(z==j){
next
}
if(m==0){
ret=cbind(data[,V[z]])
m=1
}
else{
ret=cbind(ret,data[,V[z]])
}
}
SCR2<-deviance(lm(data[,cible_col]~ret))
Fish[i]=(SCR2-SCR3)/(SCR3/(nrow(data)-length(V)-1))
i=i+1
}
df2=nrow(data)-length(V)-1
#Retrait
if((1-pf(min(Fish),1,df2))>precision_del){
print('Delete :')
print('Fisher :')
print(Fish) # Toutes les valeurs de Fisher
print('Min Fisher :')
print( min(Fish) ) # La valeur la plus petite de Fisher
print('p value :')
print((  1-pf(min(Fish),1,df2) )) # p-value de la variable sortante
print('pos :')
print( V[match(min(Fish),Fish)] ) # Position de la variable sortante
V=V[-(match(min(Fish),Fish):match(min(Fish),Fish))]
print(V)
break_=0
}
else{
print('Delete :')
print('Nothing') # Pas de valeur sortante
print('p value :')
print((  1-pf(min(Fish),1,df2) ))
print('pos :')
print( V[match(min(Fish),Fish)] )
print('Fisher :')
print(Fish) # Toutes les valeurs de Fisher
print('Min Fisher :')
print( min(Fish) ) # La valeur la plus petite de Fisher
if(break_==1){
print('Last del p value')
print(1-pf(min(Fish),1,df2)) # p-value de la dernière variable sortante
print('Last ajout p value')
print(lastaj_pvalue) # p-value de la dernière variable entrante
break
}
}
t=0
for(i in V){
if(t==0){
dev=cbind(data[,i])
t=1
}
else{
dev=cbind(dev,data[,i])
}
}
mod<-lm(data[,16]~dev)
print("R squared aj")
print(summary(mod)$adj.r.squared)#Valeur de Raju^2 
}

t=0
for(i in V){
if(t==0){
dev=cbind(data[,i])
t=1
}
else{
dev=cbind(dev,data[,i])
}
}

#Etape 1
model1 <- lm(B ~ A9 + A6 , data = data)
#Sommaire du modèle
summary(model2)
#Condition d'arret n'est pas vérifiée
#Etape 2
model2 <- lm(B ~ A9 + A6 + A2, data = data)
#Sommaire du modèle
summary(model2)
#Condition d'arret n'est pas vérifiée
#Etape 3
model3 <- lm(B ~ A9 + A6 + A2 + A8, data = data)
#Sommaire du modèle
summary(model3)
#Condition d'arret n'est pas vérifiée
#Etape 4
model4 <- lm(B ~ A9 + A6 + A2 + A8 + A1, data = data)
#Sommaire du modèle
summary(model4)
#Condition d'arret n'est pas vérifiée
#Etape 5
model5 <- lm(B ~ A9 + A6 + A2 + A8 + A1 + A14, data = data)
#Sommaire du modèle
summary(model5)
#Condition d'arret n'est pas vérifiée
#Etape 6
model6 <- lm(B ~ A9 + A6 + A2 + A8 + A1 + A14 + A3, data = data)
#Sommaire du modèle
summary(model6)
#Condition d'arret n'est pas vérifiée
#Etape 7
model7 <- lm(B ~ A9 + A6 + A2 + A8 + A1 + A14 + A3+ A7, data = data)
#Sommaire du modèle
summary(model7)
#Condition d'arret vérifiée : les varibales entrantes ont p-values>10 et les variables sortantes ont des p-values<10
################ Tests de validation 
# Test d'homoscédasticité
plot(predict(model6), resid(model6))
abline(h=0)
#Test de Normalité: shapiro
shapiro.test(resid(model6))
#Test de Normalité: ks
ks.test(resid(model6),pnorm)
#Recherche de valeurs aberrantes
seqx=seq(1,60,length=60)
abr=abs(data$B-predict(model6))/32.43  #RSE = 32.43
abline(h=2, lty=2,col=2)
#Critère AIC
#La valeur AIC pour le modèle obtenu par la méthode pas à pas de sélection des variables explicatives
AIC(model6)
#La valeur AIC du modèle obtenu par la procédure step:
AIC(modelstep)